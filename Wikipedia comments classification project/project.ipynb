{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkQI2eq7UUFA",
        "outputId": "d93ed54c-bb57-4f7d-d088-91cf80e8bb73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.17.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.1.21)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.6)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ],
      "source": [
        "pip install pandas numpy scikit-learn tensorflow keras transformers matplotlib seaborn keras-tuner requests"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Dataset Preparation"
      ],
      "metadata": {
        "id": "m0zt2UkVW39-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Preprocessing"
      ],
      "metadata": {
        "id": "m4raPZ7fW7jk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Model Design\n",
        "Model 1: LSTM with Pre-trained Embeddings"
      ],
      "metadata": {
        "id": "lsefx76WXJK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report\n",
        ")\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    BertTokenizer, BertForSequenceClassification,\n",
        "    Trainer, TrainingArguments\n",
        ")\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "3QzE_v9thvDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    train_df = pd.read_csv(\"train.csv\")\n",
        "    test_df = pd.read_csv(\"test.csv\")\n",
        "    test_labels = pd.read_csv(\"test_labels.csv\")\n",
        "\n",
        "    # Merge and filter test data\n",
        "    test_full = pd.merge(test_df, test_labels, on=\"id\")\n",
        "    mask = (test_full[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] != -1).any(axis=1)\n",
        "    scored_test = test_full[mask]\n",
        "\n",
        "    return train_df, scored_test\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"http\\S+|www\\S+|@\\w+|#\\w+|[^a-zA-Z\\s]\", \"\", str(text))\n",
        "    return text.lower().strip()\n",
        "\n",
        "# Process datasets\n",
        "train_df, test_df = load_data()\n",
        "train_df[\"cleaned_text\"] = train_df[\"comment_text\"].apply(clean_text)\n",
        "test_df[\"cleaned_text\"] = test_df[\"comment_text\"].apply(clean_text)\n",
        "\n",
        "# Split training data\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_df[\"cleaned_text\"],\n",
        "    train_df[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]],\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "CGl0Y-j3t5W8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ToxicityDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.texts = texts.reset_index(drop=True)\n",
        "        self.labels = labels.reset_index(drop=True).values\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self): return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts.iloc[idx])\n",
        "        encoding = self.tokenizer(\n",
        "            text, max_length=self.max_len,\n",
        "            padding=\"max_length\", truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        }\n",
        "\n",
        "# Initialize tokenizer and datasets\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "train_dataset = ToxicityDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = ToxicityDataset(val_texts, val_labels, tokenizer)\n",
        "test_dataset = ToxicityDataset(test_df[\"cleaned_text\"], test_df.iloc[:, 2:8], tokenizer)"
      ],
      "metadata": {
        "id": "bEjLS74ZuAtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTM(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.fc = torch.nn.Linear(hidden_dim * 2, 6)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        x = self.embedding(input_ids)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.dropout(x[:, -1, :])\n",
        "        return self.fc(x)\n",
        "\n",
        "class GRUModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=200, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = torch.nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.fc = torch.nn.Linear(hidden_dim, 6)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        x = self.embedding(input_ids)\n",
        "        x, _ = self.gru(x)\n",
        "        x = self.dropout(x[:, -1, :])\n",
        "        return self.fc(x)\n",
        "\n",
        "# BERT Model\n",
        "bert_model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=6,\n",
        "    problem_type=\"multi_label_classification\"\n",
        ").to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXLyPTf1uLrm",
        "outputId": "405ca3e5-14c7-4e3d-ed87-2a19fb11375b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, optimizer, criterion, epochs=10, patience=3):\n",
        "    best_val_loss = float('inf')\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        train_loss = epoch_loss / len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "                outputs = model(input_ids, attention_mask)\n",
        "                val_loss += criterion(outputs, labels).item()\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    return model, train_losses, val_losses\n"
      ],
      "metadata": {
        "id": "jD_s8mQRuUGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Common parameters\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Train BiLSTM\n",
        "bilstm = BiLSTM(tokenizer.vocab_size).to(device)\n",
        "bilstm_optimizer = torch.optim.AdamW(bilstm.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "bilstm, bilstm_train_loss, bilstm_val_loss = train_model(\n",
        "    bilstm, train_loader, val_loader, bilstm_optimizer, criterion\n",
        ")\n",
        "\n",
        "# Train GRU\n",
        "gru = GRUModel(tokenizer.vocab_size).to(device)\n",
        "gru_optimizer = torch.optim.AdamW(gru.parameters(), lr=1e-3)\n",
        "gru, gru_train_loss, gru_val_loss = train_model(\n",
        "    gru, train_loader, val_loader, gru_optimizer, criterion\n",
        ")\n",
        "\n",
        "# Train BERT\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=bert_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "_XU_6Kq2ug3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].cpu().numpy()\n",
        "            outputs = model(input_ids, attention_mask).sigmoid().cpu().numpy()\n",
        "            preds = (outputs > 0.5).astype(int)\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels)\n",
        "\n",
        "    results = {}\n",
        "    for i, col in enumerate([\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]):\n",
        "        results[col] = {\n",
        "            \"precision\": precision_score(all_labels[:, i], all_preds[:, i], average=\"macro\"),\n",
        "            \"recall\": recall_score(all_labels[:, i], all_preds[:, i], average=\"macro\"),\n",
        "            \"f1\": f1_score(all_labels[:, i], all_preds[:, i], average=\"macro\")\n",
        "        }\n",
        "\n",
        "    results[\"macro_avg\"] = {\n",
        "        \"precision\": np.mean([v[\"precision\"] for v in results.values()]),\n",
        "        \"recall\": np.mean([v[\"recall\"] for v in results.values()]),\n",
        "        \"f1\": np.mean([v[\"f1\"] for v in results.values()])\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "# Evaluate all models\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "print(\"BiLSTM Results:\", evaluate_model(bilstm, test_loader))\n",
        "print(\"GRU Results:\", evaluate_model(gru, test_loader))\n",
        "print(\"BERT Results:\", evaluate_model(bert_model, test_loader))"
      ],
      "metadata": {
        "id": "ArgxNX5rvkQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_iiif_annotations(df, num_samples=10):\n",
        "    annotations = []\n",
        "    for _, row in df.sample(num_samples).iterrows():\n",
        "        annotation = {\n",
        "            \"@context\": \"http://iiif.io/api/presentation/3/context.json\",\n",
        "            \"id\": f\"https://example.org/annotation/{row['id']}\",\n",
        "            \"type\": \"Annotation\",\n",
        "            \"motivation\": \"classifying\",\n",
        "            \"body\": {\n",
        "                \"type\": \"TextualBody\",\n",
        "                \"value\": str(row[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].to_dict()),\n",
        "                \"format\": \"text/plain\"\n",
        "            },\n",
        "            \"target\": {\n",
        "                \"source\": f\"https://example.org/wiki_comments/{row['id']}.jpg\",\n",
        "                \"selector\": {\"type\": \"FragmentSelector\", \"value\": \"xywh=0,0,500,500\"}\n",
        "            }\n",
        "        }\n",
        "        annotations.append(annotation)\n",
        "    return annotations\n",
        "\n",
        "with open(\"iiif_annotations.json\", \"w\") as f:\n",
        "    json.dump(generate_iiif_annotations(test_df), f)"
      ],
      "metadata": {
        "id": "fj9JChctGb3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [9] Error Analysis\n",
        "def analyze_errors(model, test_loader):\n",
        "    model.eval()\n",
        "    errors = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].cpu().numpy()\n",
        "            outputs = model(input_ids, attention_mask).sigmoid().cpu().numpy()\n",
        "            preds = (outputs > 0.5).astype(int)\n",
        "\n",
        "            for i in range(len(preds)):\n",
        "                if not np.array_equal(preds[i], labels[i]):\n",
        "                    errors.append({\n",
        "                        \"text\": test_df[\"cleaned_text\"].iloc[i],\n",
        "                        \"true\": labels[i],\n",
        "                        \"pred\": preds[i]\n",
        "                    })\n",
        "    return errors[:10]  # Return first 10 errors\n",
        "\n",
        "print(\"BERT Error Examples:\", analyze_errors(bert_model, test_loader))\n"
      ],
      "metadata": {
        "id": "CHGTht2WGjak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [10] Visualization\n",
        "def plot_metrics(model_name, train_loss, val_loss):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_loss, label=f\"{model_name} Training Loss\")\n",
        "    plt.plot(val_loss, label=f\"{model_name} Validation Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_metrics(\"BiLSTM\", bilstm_train_loss, bilstm_val_loss)\n",
        "plot_metrics(\"GRU\", gru_train_loss, gru_val_loss)"
      ],
      "metadata": {
        "id": "DmsUR4--GoTF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}